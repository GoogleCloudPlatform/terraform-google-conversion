// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This code is generated by Magic Modules using the following:
//
//     Configuration: https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/products/dataproc/Batch.yaml
//     Template:      https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/templates/tgc_next/services/resource.go.tmpl
//
//     DO NOT EDIT this file directly. Any changes made to this file will be
//     overwritten during the next generation cycle.
//
// ----------------------------------------------------------------------------

package dataproc

import (
	"bytes"
	"context"
	"fmt"
	"log"
	"reflect"
	"regexp"
	"sort"
	"strconv"
	"strings"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"

	"github.com/GoogleCloudPlatform/terraform-google-conversion/v7/pkg/tgcresource"
	"github.com/GoogleCloudPlatform/terraform-google-conversion/v7/pkg/tpgresource"
	transport_tpg "github.com/GoogleCloudPlatform/terraform-google-conversion/v7/pkg/transport"
	"github.com/GoogleCloudPlatform/terraform-google-conversion/v7/pkg/verify"
)

/*
 * Dataproc Batch api apends subminor version to the provided
 * version. We are suppressing this server generated subminor.
 */
func CloudDataprocBatchRuntimeConfigVersionDiffSuppressFunc(old, new string) bool {
	if old != "" && strings.HasPrefix(new, old) || (new != "" && strings.HasPrefix(old, new)) {
		return true
	}

	return old == new
}

func CloudDataprocBatchRuntimeConfigVersionDiffSuppress(_, old, new string, d *schema.ResourceData) bool {
	return CloudDataprocBatchRuntimeConfigVersionDiffSuppressFunc(old, new)
}

var (
	_ = bytes.Clone
	_ = context.WithCancel
	_ = fmt.Sprintf
	_ = log.Print
	_ = reflect.ValueOf
	_ = regexp.Match
	_ = sort.IntSlice{}
	_ = strconv.Atoi
	_ = strings.Trim
	_ = schema.Noop
	_ = validation.All
	_ = tgcresource.RemoveTerraformAttributionLabel
	_ = tpgresource.GetRegion
	_ = transport_tpg.Config{}
	_ = verify.ProjectRegex
)

const DataprocBatchAssetType string = "dataproc.googleapis.com/Batch"

const DataprocBatchSchemaName string = "google_dataproc_batch"

func ResourceDataprocBatch() *schema.Resource {
	return &schema.Resource{
		Schema: map[string]*schema.Schema{
			"batch_id": {
				Type:     schema.TypeString,
				Optional: true,
				ForceNew: true,
				Description: `The ID to use for the batch, which will become the final component of the batch's resource name.
This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.`,
			},
			"environment_config": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `Environment configuration for the batch execution.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"execution_config": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Execution configuration for a workload.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"authentication_config": {
										Type:        schema.TypeList,
										Optional:    true,
										ForceNew:    true,
										Description: `Authentication configuration for a workload is used to set the default identity for the workload execution.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"user_workload_authentication_type": {
													Type:         schema.TypeString,
													Optional:     true,
													ForceNew:     true,
													ValidateFunc: verify.ValidateEnum([]string{"SERVICE_ACCOUNT", "END_USER_CREDENTIALS", ""}),
													Description:  `Authentication type for the user workload running in containers. Possible values: ["SERVICE_ACCOUNT", "END_USER_CREDENTIALS"]`,
												},
											},
										},
									},
									"kms_key": {
										Type:        schema.TypeString,
										Optional:    true,
										ForceNew:    true,
										Description: `The Cloud KMS key to use for encryption.`,
									},
									"network_tags": {
										Type:        schema.TypeList,
										Optional:    true,
										ForceNew:    true,
										Description: `Tags used for network traffic control.`,
										Elem: &schema.Schema{
											Type: schema.TypeString,
										},
									},
									"network_uri": {
										Type:          schema.TypeString,
										Optional:      true,
										ForceNew:      true,
										Description:   `Network configuration for workload execution.`,
										ConflictsWith: []string{"environment_config.0.execution_config.0.subnetwork_uri"},
									},
									"service_account": {
										Type:        schema.TypeString,
										Computed:    true,
										Optional:    true,
										ForceNew:    true,
										Description: `Service account that used to execute workload.`,
									},
									"staging_bucket": {
										Type:     schema.TypeString,
										Optional: true,
										ForceNew: true,
										Description: `A Cloud Storage bucket used to stage workload dependencies, config files, and store
workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket,
Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running,
and then create and manage project-level, per-location staging and temporary buckets.
This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.`,
									},
									"subnetwork_uri": {
										Type:          schema.TypeString,
										Optional:      true,
										ForceNew:      true,
										Description:   `Subnetwork configuration for workload execution.`,
										ConflictsWith: []string{"environment_config.0.execution_config.0.network_uri"},
									},
									"ttl": {
										Type:     schema.TypeString,
										Computed: true,
										Optional: true,
										ForceNew: true,
										Description: `The duration after which the workload will be terminated.
When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing
work to finish. If ttl is not specified for a batch workload, the workload will be allowed to run until it
exits naturally (or run forever without exiting). If ttl is not specified for an interactive session,
it defaults to 24 hours. If ttl is not specified for a batch that uses 2.1+ runtime version, it defaults to 4 hours.
Minimum value is 10 minutes; maximum value is 14 days. If both ttl and idleTtl are specified (for an interactive session),
the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idleTtl or
when ttl has been exceeded, whichever occurs first.`,
									},
								},
							},
						},
						"peripherals_config": {
							Type:        schema.TypeList,
							Computed:    true,
							Optional:    true,
							ForceNew:    true,
							Description: `Peripherals configuration that workload has access to.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"metastore_service": {
										Type:        schema.TypeString,
										Optional:    true,
										ForceNew:    true,
										Description: `Resource name of an existing Dataproc Metastore service.`,
									},
									"spark_history_server_config": {
										Type:        schema.TypeList,
										Optional:    true,
										ForceNew:    true,
										Description: `The Spark History Server configuration for the workload.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"dataproc_cluster": {
													Type:        schema.TypeString,
													Optional:    true,
													ForceNew:    true,
													Description: `Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.`,
												},
											},
										},
									},
								},
							},
						},
					},
				},
			},
			"labels": {
				Type:     schema.TypeMap,
				Optional: true,
				Description: `The labels to associate with this batch.


**Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
Please refer to the field 'effective_labels' for all of the labels present on the resource.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"location": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `The location in which the batch will be created in.`,
			},
			"pyspark_batch": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `PySpark batch config.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"archive_uris": {
							Type:     schema.TypeList,
							Optional: true,
							ForceNew: true,
							Description: `HCFS URIs of archives to be extracted into the working directory of each executor.
Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"args": {
							Type:     schema.TypeList,
							Optional: true,
							ForceNew: true,
							Description: `The arguments to pass to the driver. Do not include arguments that can be set as batch
properties, such as --conf, since a collision can occur that causes an incorrect batch submission.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"file_uris": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `HCFS URIs of files to be placed in the working directory of each executor.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"jar_file_uris": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"main_python_file_uri": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.`,
						},
						"python_file_uris": {
							Type:     schema.TypeList,
							Optional: true,
							ForceNew: true,
							Description: `HCFS file URIs of Python files to pass to the PySpark framework.
Supported file types: .py, .egg, and .zip.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
					},
				},
				ExactlyOneOf: []string{"pyspark_batch", "spark_batch", "spark_sql_batch", "spark_r_batch"},
			},
			"runtime_config": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `Runtime configuration for the batch execution.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"autotuning_config": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Optional. Autotuning configuration of the workload.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"scenarios": {
										Type:        schema.TypeList,
										Optional:    true,
										ForceNew:    true,
										Description: `Optional. Scenarios for which tunings are applied. Possible values: ["SCALING", "BROADCAST_HASH_JOIN", "MEMORY"]`,
										Elem: &schema.Schema{
											Type:         schema.TypeString,
											ValidateFunc: verify.ValidateEnum([]string{"SCALING", "BROADCAST_HASH_JOIN", "MEMORY"}),
										},
										RequiredWith: []string{"runtime_config.0.cohort"},
									},
								},
							},
						},
						"cohort": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Optional. Cohort identifier. Identifies families of the workloads having the same shape, e.g. daily ETL jobs.`,
						},
						"container_image": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Optional custom container image for the job runtime environment. If not specified, a default container image will be used.`,
						},
						"properties": {
							Type:        schema.TypeMap,
							Optional:    true,
							ForceNew:    true,
							Description: `A mapping of property names to values, which are used to configure workload execution.`,
							Elem:        &schema.Schema{Type: schema.TypeString},
						},
						"version": {
							Type:             schema.TypeString,
							Computed:         true,
							Optional:         true,
							ForceNew:         true,
							DiffSuppressFunc: CloudDataprocBatchRuntimeConfigVersionDiffSuppress,
							Description:      `Version of the batch runtime.`,
						},
					},
				},
			},
			"spark_batch": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `Spark batch config.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"archive_uris": {
							Type:     schema.TypeList,
							Optional: true,
							ForceNew: true,
							Description: `HCFS URIs of archives to be extracted into the working directory of each executor.
Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"args": {
							Type:     schema.TypeList,
							Optional: true,
							ForceNew: true,
							Description: `The arguments to pass to the driver. Do not include arguments that can be set as batch
properties, such as --conf, since a collision can occur that causes an incorrect batch submission.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"file_uris": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `HCFS URIs of files to be placed in the working directory of each executor.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"jar_file_uris": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"main_class": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `The name of the driver main class. The jar file that contains the class must be in the
classpath or specified in jarFileUris.`,
							ExactlyOneOf: []string{"spark_batch.0.main_jar_file_uri"},
						},
						"main_jar_file_uri": {
							Type:         schema.TypeString,
							Optional:     true,
							ForceNew:     true,
							Description:  `The HCFS URI of the jar file that contains the main class.`,
							ExactlyOneOf: []string{"spark_batch.0.main_class"},
						},
					},
				},
				ExactlyOneOf: []string{"pyspark_batch", "spark_batch", "spark_sql_batch", "spark_r_batch"},
			},
			"spark_r_batch": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `SparkR batch config.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"archive_uris": {
							Type:     schema.TypeList,
							Optional: true,
							ForceNew: true,
							Description: `HCFS URIs of archives to be extracted into the working directory of each executor.
Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"args": {
							Type:     schema.TypeList,
							Optional: true,
							ForceNew: true,
							Description: `The arguments to pass to the driver. Do not include arguments that can be set as batch
properties, such as --conf, since a collision can occur that causes an incorrect batch submission.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"file_uris": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `HCFS URIs of files to be placed in the working directory of each executor.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"main_r_file_uri": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.`,
						},
					},
				},
				ExactlyOneOf: []string{"pyspark_batch", "spark_batch", "spark_sql_batch", "spark_r_batch"},
			},
			"spark_sql_batch": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `Spark SQL batch config.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"jar_file_uris": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `HCFS URIs of jar files to be added to the Spark CLASSPATH.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"query_file_uri": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `The HCFS URI of the script that contains Spark SQL queries to execute.`,
						},
						"query_variables": {
							Type:        schema.TypeMap,
							Optional:    true,
							ForceNew:    true,
							Description: `Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).`,
							Elem:        &schema.Schema{Type: schema.TypeString},
						},
					},
				},
				ExactlyOneOf: []string{"pyspark_batch", "spark_batch", "spark_sql_batch", "spark_r_batch"},
			},
			"create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The time when the batch was created.`,
			},
			"creator": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The email address of the user who created the batch.`,
			},
			"effective_labels": {
				Type:        schema.TypeMap,
				Computed:    true,
				ForceNew:    true,
				Description: `All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other clients and services.`,
				Elem:        &schema.Schema{Type: schema.TypeString},
			},
			"name": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The resource name of the batch.`,
			},
			"operation": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The resource name of the operation associated with this batch.`,
			},
			"runtime_info": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `Runtime information about batch execution.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{},
				},
			},
			"state": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The state of the batch. For possible values, see the [API documentation](https://cloud.google.com/dataproc-serverless/docs/reference/rest/v1/projects.locations.batches#State).`,
			},
			"state_history": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `Historical state information for the batch.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{},
				},
			},
			"state_message": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Batch state details, such as a failure description if the state is FAILED.`,
			},
			"state_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Batch state details, such as a failure description if the state is FAILED.`,
			},
			"terraform_labels": {
				Type:     schema.TypeMap,
				Computed: true,
				Description: `The combination of labels configured directly on the resource
 and default labels configured on the provider.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"uuid": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `A batch UUID (Unique Universal Identifier). The service generates this value when it creates the batch.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}
