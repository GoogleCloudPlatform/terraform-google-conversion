// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This code is generated by Magic Modules using the following:
//
//     Configuration: https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/products/monitoring/AlertPolicy.yaml
//     Template:      https://github.com/GoogleCloudPlatform/magic-modules/tree/main/mmv1/templates/tgc_next/services/resource.go.tmpl
//
//     DO NOT EDIT this file directly. Any changes made to this file will be
//     overwritten during the next generation cycle.
//
// ----------------------------------------------------------------------------

package monitoring

import (
	"bytes"
	"context"
	"fmt"
	"log"
	"reflect"
	"regexp"
	"sort"
	"strconv"
	"strings"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/structure"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"

	"github.com/GoogleCloudPlatform/terraform-google-conversion/v7/pkg/tgcresource"
	"github.com/GoogleCloudPlatform/terraform-google-conversion/v7/pkg/tpgresource"
	transport_tpg "github.com/GoogleCloudPlatform/terraform-google-conversion/v7/pkg/transport"
	"github.com/GoogleCloudPlatform/terraform-google-conversion/v7/pkg/verify"
)

// API does not return a value for REDUCE_NONE
func crossSeriesReducerDiffSuppress(k, old, new string, d *schema.ResourceData) bool {
	return (new == "" && old == "REDUCE_NONE") || (new == "REDUCE_NONE" && old == "")
}

var (
	_ = bytes.Clone
	_ = context.WithCancel
	_ = fmt.Sprintf
	_ = log.Print
	_ = reflect.ValueOf
	_ = regexp.Match
	_ = sort.IntSlice{}
	_ = strconv.Atoi
	_ = strings.Trim
	_ = schema.Noop
	_ = structure.NormalizeJsonString
	_ = validation.All
	_ = tgcresource.RemoveTerraformAttributionLabel
	_ = tpgresource.GetRegion
	_ = transport_tpg.Config{}
	_ = verify.ProjectRegex
)

const MonitoringAlertPolicyAssetType string = "monitoring.googleapis.com/AlertPolicy"

const MonitoringAlertPolicySchemaName string = "google_monitoring_alert_policy"

func ResourceMonitoringAlertPolicy() *schema.Resource {
	return &schema.Resource{
		Schema: map[string]*schema.Schema{
			"combiner": {
				Type:         schema.TypeString,
				Required:     true,
				ValidateFunc: verify.ValidateEnum([]string{"AND", "OR", "AND_WITH_MATCHING_RESOURCE"}),
				Description: `How to combine the results of multiple conditions to
determine if an incident should be opened. Possible values: ["AND", "OR", "AND_WITH_MATCHING_RESOURCE"]`,
			},
			"conditions": {
				Type:     schema.TypeList,
				Required: true,
				Description: `A list of conditions for the policy. The conditions are combined by
AND or OR according to the combiner field. If the combined conditions
evaluate to true, then an incident is created. A policy can have from
one to six conditions.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"display_name": {
							Type:     schema.TypeString,
							Required: true,
							Description: `A short name or phrase used to identify the
condition in dashboards, notifications, and
incidents. To avoid confusion, don't use the same
display name for multiple conditions in the same
policy.`,
						},
						"condition_absent": {
							Type:     schema.TypeList,
							Optional: true,
							Description: `A condition that checks that a time series
continues to receive new data points.`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"duration": {
										Type:     schema.TypeString,
										Required: true,
										Description: `The amount of time that a time series must
fail to report new data to be considered
failing. Currently, only values that are a
multiple of a minute--e.g. 60s, 120s, or 300s
--are supported.`,
									},
									"aggregations": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `Specifies the alignment of data points in
individual time series as well as how to
combine the retrieved time series together
(such as when aggregating multiple streams
on each resource to a single stream for each
resource or when aggregating streams across
all members of a group of resources).
Multiple aggregations are applied in the
order specified.`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"alignment_period": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `The alignment period for per-time
series alignment. If present,
alignmentPeriod must be at least
60 seconds. After per-time series
alignment, each time series will
contain data points only on the
period boundaries. If
perSeriesAligner is not specified
or equals ALIGN_NONE, then this
field is ignored. If
perSeriesAligner is specified and
does not equal ALIGN_NONE, then
this field must be defined;
otherwise an error is returned.`,
												},
												"cross_series_reducer": {
													Type:             schema.TypeString,
													Optional:         true,
													ValidateFunc:     verify.ValidateEnum([]string{"REDUCE_NONE", "REDUCE_MEAN", "REDUCE_MIN", "REDUCE_MAX", "REDUCE_SUM", "REDUCE_STDDEV", "REDUCE_COUNT", "REDUCE_COUNT_TRUE", "REDUCE_COUNT_FALSE", "REDUCE_FRACTION_TRUE", "REDUCE_PERCENTILE_99", "REDUCE_PERCENTILE_95", "REDUCE_PERCENTILE_50", "REDUCE_PERCENTILE_05", ""}),
													DiffSuppressFunc: crossSeriesReducerDiffSuppress,
													Description: `The approach to be used to combine
time series. Not all reducer
functions may be applied to all
time series, depending on the
metric type and the value type of
the original time series.
Reduction may change the metric
type of value type of the time
series.Time series data must be
aligned in order to perform cross-
time series reduction. If
crossSeriesReducer is specified,
then perSeriesAligner must be
specified and not equal ALIGN_NONE
and alignmentPeriod must be
specified; otherwise, an error is
returned. Possible values: ["REDUCE_NONE", "REDUCE_MEAN", "REDUCE_MIN", "REDUCE_MAX", "REDUCE_SUM", "REDUCE_STDDEV", "REDUCE_COUNT", "REDUCE_COUNT_TRUE", "REDUCE_COUNT_FALSE", "REDUCE_FRACTION_TRUE", "REDUCE_PERCENTILE_99", "REDUCE_PERCENTILE_95", "REDUCE_PERCENTILE_50", "REDUCE_PERCENTILE_05"]`,
												},
												"group_by_fields": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `The set of fields to preserve when
crossSeriesReducer is specified.
The groupByFields determine how
the time series are partitioned
into subsets prior to applying the
aggregation function. Each subset
contains time series that have the
same value for each of the
grouping fields. Each individual
time series is a member of exactly
one subset. The crossSeriesReducer
is applied to each subset of time
series. It is not possible to
reduce across different resource
types, so this field implicitly
contains resource.type. Fields not
specified in groupByFields are
aggregated away. If groupByFields
is not specified and all the time
series have the same resource
type, then the time series are
aggregated into a single output
time series. If crossSeriesReducer
is not defined, this field is
ignored.`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
												"per_series_aligner": {
													Type:         schema.TypeString,
													Optional:     true,
													ValidateFunc: verify.ValidateEnum([]string{"ALIGN_NONE", "ALIGN_DELTA", "ALIGN_RATE", "ALIGN_INTERPOLATE", "ALIGN_NEXT_OLDER", "ALIGN_MIN", "ALIGN_MAX", "ALIGN_MEAN", "ALIGN_COUNT", "ALIGN_SUM", "ALIGN_STDDEV", "ALIGN_COUNT_TRUE", "ALIGN_COUNT_FALSE", "ALIGN_FRACTION_TRUE", "ALIGN_PERCENTILE_99", "ALIGN_PERCENTILE_95", "ALIGN_PERCENTILE_50", "ALIGN_PERCENTILE_05", "ALIGN_PERCENT_CHANGE", ""}),
													Description: `The approach to be used to align
individual time series. Not all
alignment functions may be applied
to all time series, depending on
the metric type and value type of
the original time series.
Alignment may change the metric
type or the value type of the time
series.Time series data must be
aligned in order to perform cross-
time series reduction. If
crossSeriesReducer is specified,
then perSeriesAligner must be
specified and not equal ALIGN_NONE
and alignmentPeriod must be
specified; otherwise, an error is
returned. Possible values: ["ALIGN_NONE", "ALIGN_DELTA", "ALIGN_RATE", "ALIGN_INTERPOLATE", "ALIGN_NEXT_OLDER", "ALIGN_MIN", "ALIGN_MAX", "ALIGN_MEAN", "ALIGN_COUNT", "ALIGN_SUM", "ALIGN_STDDEV", "ALIGN_COUNT_TRUE", "ALIGN_COUNT_FALSE", "ALIGN_FRACTION_TRUE", "ALIGN_PERCENTILE_99", "ALIGN_PERCENTILE_95", "ALIGN_PERCENTILE_50", "ALIGN_PERCENTILE_05", "ALIGN_PERCENT_CHANGE"]`,
												},
											},
										},
									},
									"filter": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `A filter that identifies which time series
should be compared with the threshold.The
filter is similar to the one that is
specified in the
MetricService.ListTimeSeries request (that
call is useful to verify the time series
that will be retrieved / processed) and must
specify the metric type and optionally may
contain restrictions on resource type,
resource labels, and metric labels. This
field may not exceed 2048 Unicode characters
in length.`,
									},
									"trigger": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `The number/percent of time series for which
the comparison must hold in order for the
condition to trigger. If unspecified, then
the condition will trigger if the comparison
is true for any of the time series that have
been identified by filter and aggregations.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"count": {
													Type:     schema.TypeInt,
													Optional: true,
													Description: `The absolute number of time series
that must fail the predicate for the
condition to be triggered.`,
												},
												"percent": {
													Type:     schema.TypeFloat,
													Optional: true,
													Description: `The percentage of time series that
must fail the predicate for the
condition to be triggered.`,
												},
											},
										},
									},
								},
							},
						},
						"condition_matched_log": {
							Type:     schema.TypeList,
							Optional: true,
							Description: `A condition that checks for log messages matching given constraints.
If set, no other conditions can be present.`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"filter": {
										Type:        schema.TypeString,
										Required:    true,
										Description: `A logs-based filter.`,
									},
									"label_extractors": {
										Type:     schema.TypeMap,
										Optional: true,
										Description: `A map from a label key to an extractor expression, which is used to
extract the value for this label key. Each entry in this map is
a specification for how data should be extracted from log entries that
match filter. Each combination of extracted values is treated as
a separate rule for the purposes of triggering notifications.
Label keys and corresponding values can be used in notifications
generated by this condition.`,
										Elem: &schema.Schema{Type: schema.TypeString},
									},
								},
							},
						},
						"condition_monitoring_query_language": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `A Monitoring Query Language query that outputs a boolean stream`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"duration": {
										Type:     schema.TypeString,
										Required: true,
										Description: `The amount of time that a time series must
violate the threshold to be considered
failing. Currently, only values that are a
multiple of a minute--e.g., 0, 60, 120, or
300 seconds--are supported. If an invalid
value is given, an error will be returned.
When choosing a duration, it is useful to
keep in mind the frequency of the underlying
time series data (which may also be affected
by any alignments specified in the
aggregations field); a good duration is long
enough so that a single outlier does not
generate spurious alerts, but short enough
that unhealthy states are detected and
alerted on quickly.`,
									},
									"query": {
										Type:        schema.TypeString,
										Required:    true,
										Description: `Monitoring Query Language query that outputs a boolean stream.`,
									},
									"evaluation_missing_data": {
										Type:         schema.TypeString,
										Optional:     true,
										ValidateFunc: verify.ValidateEnum([]string{"EVALUATION_MISSING_DATA_INACTIVE", "EVALUATION_MISSING_DATA_ACTIVE", "EVALUATION_MISSING_DATA_NO_OP", ""}),
										Description: `A condition control that determines how
metric-threshold conditions are evaluated when
data stops arriving. Possible values: ["EVALUATION_MISSING_DATA_INACTIVE", "EVALUATION_MISSING_DATA_ACTIVE", "EVALUATION_MISSING_DATA_NO_OP"]`,
									},
									"trigger": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `The number/percent of time series for which
the comparison must hold in order for the
condition to trigger. If unspecified, then
the condition will trigger if the comparison
is true for any of the time series that have
been identified by filter and aggregations,
or by the ratio, if denominator_filter and
denominator_aggregations are specified.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"count": {
													Type:     schema.TypeInt,
													Optional: true,
													Description: `The absolute number of time series
that must fail the predicate for the
condition to be triggered.`,
												},
												"percent": {
													Type:     schema.TypeFloat,
													Optional: true,
													Description: `The percentage of time series that
must fail the predicate for the
condition to be triggered.`,
												},
											},
										},
									},
								},
							},
						},
						"condition_prometheus_query_language": {
							Type:     schema.TypeList,
							Optional: true,
							Description: `A condition type that allows alert policies to be defined using
Prometheus Query Language (PromQL).

The PrometheusQueryLanguageCondition message contains information
from a Prometheus alerting rule and its associated rule group.`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"query": {
										Type:     schema.TypeString,
										Required: true,
										Description: `The PromQL expression to evaluate. Every evaluation cycle this
expression is evaluated at the current time, and all resultant time
series become pending/firing alerts. This field must not be empty.`,
									},
									"alert_rule": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `The alerting rule name of this alert in the corresponding Prometheus
configuration file.

Some external tools may require this field to be populated correctly
in order to refer to the original Prometheus configuration file.
The rule group name and the alert name are necessary to update the
relevant AlertPolicies in case the definition of the rule group changes
in the future.

This field is optional. If this field is not empty, then it must be a
valid Prometheus label name.`,
									},
									"disable_metric_validation": {
										Type:     schema.TypeBool,
										Optional: true,
										Description: `Whether to disable metric existence validation for this condition.

This allows alerting policies to be defined on metrics that do not yet
exist, improving advanced customer workflows such as configuring
alerting policies using Terraform.

Users with the 'monitoring.alertPolicyViewer' role are able to see the
name of the non-existent metric in the alerting policy condition.`,
									},
									"duration": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `Alerts are considered firing once their PromQL expression evaluated
to be "true" for this long. Alerts whose PromQL expression was not
evaluated to be "true" for long enough are considered pending. The
default value is zero. Must be zero or positive.`,
									},
									"evaluation_interval": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `How often this rule should be evaluated. Must be a positive multiple
of 30 seconds or missing. The default value is 30 seconds. If this
PrometheusQueryLanguageCondition was generated from a Prometheus
alerting rule, then this value should be taken from the enclosing
rule group.`,
									},
									"labels": {
										Type:     schema.TypeMap,
										Optional: true,
										Description: `Labels to add to or overwrite in the PromQL query result. Label names
must be valid.

Label values can be templatized by using variables. The only available
variable names are the names of the labels in the PromQL result,
although label names beginning with \_\_ (two "\_") are reserved for
internal use. "labels" may be empty. This field is intended to be used
for organizing and identifying the AlertPolicy.`,
										Elem: &schema.Schema{Type: schema.TypeString},
									},
									"rule_group": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `The rule group name of this alert in the corresponding Prometheus
configuration file.

Some external tools may require this field to be populated correctly
in order to refer to the original Prometheus configuration file.
The rule group name and the alert name are necessary to update the
relevant AlertPolicies in case the definition of the rule group changes
in the future. This field is optional.`,
									},
								},
							},
						},
						"condition_sql": {
							Type:     schema.TypeList,
							Optional: true,
							Description: `A condition that allows alerting policies to be defined using GoogleSQL.
SQL conditions examine a sliding window of logs using GoogleSQL.
Alert policies with SQL conditions may incur additional billing.`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"query": {
										Type:     schema.TypeString,
										Required: true,
										Description: `The Log Analytics SQL query to run, as a string.  The query must
conform to the required shape. Specifically, the query must not try to
filter the input by time.  A filter will automatically be applied
to filter the input so that the query receives all rows received
since the last time the query was run.`,
									},
									"boolean_test": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `A test that uses an alerting result in a boolean column produced by the SQL query.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"column": {
													Type:     schema.TypeString,
													Required: true,
													Description: `The name of the column containing the boolean value. If the value in a row is
NULL, that row is ignored.`,
												},
											},
										},
										ExactlyOneOf: []string{},
									},
									"daily": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Used to schedule the query to run every so many days.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"periodicity": {
													Type:     schema.TypeInt,
													Required: true,
													Description: `The number of days between runs. Must be greater than or equal
to 1 day and less than or equal to 30 days.`,
												},
												"execution_time": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `The time of day (in UTC) at which the query should run. If left
unspecified, the server picks an arbitrary time of day and runs
the query at the same time each day.`,
													MaxItems: 1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"hours": {
																Type:     schema.TypeInt,
																Optional: true,
																Description: `Hours of a day in 24 hour format. Must be greater than or equal
to 0 and typically must be less than or equal to 23. An API may
choose to allow the value "24:00:00" for scenarios like business
closing time.`,
															},
															"minutes": {
																Type:     schema.TypeInt,
																Optional: true,
																Description: `Minutes of an hour. Must be greater than or equal to 0 and
less than or equal to 59.`,
															},
															"nanos": {
																Type:     schema.TypeInt,
																Optional: true,
																Description: `Fractions of seconds, in nanoseconds. Must be greater than or
equal to 0 and less than or equal to 999,999,999.`,
															},
															"seconds": {
																Type:     schema.TypeInt,
																Optional: true,
																Description: `Seconds of a minute. Must be greater than or equal to 0 and
typically must be less than or equal to 59. An API may allow the
value 60 if it allows leap-seconds.`,
															},
														},
													},
												},
											},
										},
										ExactlyOneOf: []string{},
									},
									"hourly": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Used to schedule the query to run every so many hours.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"periodicity": {
													Type:     schema.TypeInt,
													Required: true,
													Description: `Number of hours between runs. The interval must be greater than or
equal to 1 hour and less than or equal to 48 hours.`,
												},
												"minute_offset": {
													Type:     schema.TypeInt,
													Optional: true,
													Description: `The number of minutes after the hour (in UTC) to run the query.
Must be greater than or equal to 0 minutes and less than or equal to
59 minutes.  If left unspecified, then an arbitrary offset is used.`,
												},
											},
										},
										ExactlyOneOf: []string{},
									},
									"minutes": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Used to schedule the query to run every so many minutes.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"periodicity": {
													Type:     schema.TypeInt,
													Required: true,
													Description: `Number of minutes between runs. The interval must be greater than or
equal to 5 minutes and less than or equal to 1440 minutes.`,
												},
											},
										},
										ExactlyOneOf: []string{},
									},
									"row_count_test": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `A test that checks if the number of rows in the result set violates some threshold.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"comparison": {
													Type:         schema.TypeString,
													Required:     true,
													ValidateFunc: verify.ValidateEnum([]string{"COMPARISON_GT", "COMPARISON_GE", "COMPARISON_LT", "COMPARISON_LE", "COMPARISON_EQ", "COMPARISON_NE"}),
													Description: `The comparison to apply between the time
series (indicated by filter and aggregation)
and the threshold (indicated by
threshold_value). The comparison is applied
on each time series, with the time series on
the left-hand side and the threshold on the
right-hand side. Only COMPARISON_LT and
COMPARISON_GT are supported currently. Possible values: ["COMPARISON_GT", "COMPARISON_GE", "COMPARISON_LT", "COMPARISON_LE", "COMPARISON_EQ", "COMPARISON_NE"]`,
												},
												"threshold": {
													Type:        schema.TypeInt,
													Required:    true,
													Description: `The value against which to compare the row count.`,
												},
											},
										},
										ExactlyOneOf: []string{},
									},
								},
							},
						},
						"condition_threshold": {
							Type:     schema.TypeList,
							Optional: true,
							Description: `A condition that compares a time series against a
threshold.`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"comparison": {
										Type:         schema.TypeString,
										Required:     true,
										ValidateFunc: verify.ValidateEnum([]string{"COMPARISON_GT", "COMPARISON_GE", "COMPARISON_LT", "COMPARISON_LE", "COMPARISON_EQ", "COMPARISON_NE"}),
										Description: `The comparison to apply between the time
series (indicated by filter and aggregation)
and the threshold (indicated by
threshold_value). The comparison is applied
on each time series, with the time series on
the left-hand side and the threshold on the
right-hand side. Only COMPARISON_LT and
COMPARISON_GT are supported currently. Possible values: ["COMPARISON_GT", "COMPARISON_GE", "COMPARISON_LT", "COMPARISON_LE", "COMPARISON_EQ", "COMPARISON_NE"]`,
									},
									"duration": {
										Type:     schema.TypeString,
										Required: true,
										Description: `The amount of time that a time series must
violate the threshold to be considered
failing. Currently, only values that are a
multiple of a minute--e.g., 0, 60, 120, or
300 seconds--are supported. If an invalid
value is given, an error will be returned.
When choosing a duration, it is useful to
keep in mind the frequency of the underlying
time series data (which may also be affected
by any alignments specified in the
aggregations field); a good duration is long
enough so that a single outlier does not
generate spurious alerts, but short enough
that unhealthy states are detected and
alerted on quickly.`,
									},
									"aggregations": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `Specifies the alignment of data points in
individual time series as well as how to
combine the retrieved time series together
(such as when aggregating multiple streams
on each resource to a single stream for each
resource or when aggregating streams across
all members of a group of resources).
Multiple aggregations are applied in the
order specified.This field is similar to the
one in the MetricService.ListTimeSeries
request. It is advisable to use the
ListTimeSeries method when debugging this
field.`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"alignment_period": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `The alignment period for per-time
series alignment. If present,
alignmentPeriod must be at least
60 seconds. After per-time series
alignment, each time series will
contain data points only on the
period boundaries. If
perSeriesAligner is not specified
or equals ALIGN_NONE, then this
field is ignored. If
perSeriesAligner is specified and
does not equal ALIGN_NONE, then
this field must be defined;
otherwise an error is returned.`,
												},
												"cross_series_reducer": {
													Type:             schema.TypeString,
													Optional:         true,
													ValidateFunc:     verify.ValidateEnum([]string{"REDUCE_NONE", "REDUCE_MEAN", "REDUCE_MIN", "REDUCE_MAX", "REDUCE_SUM", "REDUCE_STDDEV", "REDUCE_COUNT", "REDUCE_COUNT_TRUE", "REDUCE_COUNT_FALSE", "REDUCE_FRACTION_TRUE", "REDUCE_PERCENTILE_99", "REDUCE_PERCENTILE_95", "REDUCE_PERCENTILE_50", "REDUCE_PERCENTILE_05", ""}),
													DiffSuppressFunc: crossSeriesReducerDiffSuppress,
													Description: `The approach to be used to combine
time series. Not all reducer
functions may be applied to all
time series, depending on the
metric type and the value type of
the original time series.
Reduction may change the metric
type of value type of the time
series.Time series data must be
aligned in order to perform cross-
time series reduction. If
crossSeriesReducer is specified,
then perSeriesAligner must be
specified and not equal ALIGN_NONE
and alignmentPeriod must be
specified; otherwise, an error is
returned. Possible values: ["REDUCE_NONE", "REDUCE_MEAN", "REDUCE_MIN", "REDUCE_MAX", "REDUCE_SUM", "REDUCE_STDDEV", "REDUCE_COUNT", "REDUCE_COUNT_TRUE", "REDUCE_COUNT_FALSE", "REDUCE_FRACTION_TRUE", "REDUCE_PERCENTILE_99", "REDUCE_PERCENTILE_95", "REDUCE_PERCENTILE_50", "REDUCE_PERCENTILE_05"]`,
												},
												"group_by_fields": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `The set of fields to preserve when
crossSeriesReducer is specified.
The groupByFields determine how
the time series are partitioned
into subsets prior to applying the
aggregation function. Each subset
contains time series that have the
same value for each of the
grouping fields. Each individual
time series is a member of exactly
one subset. The crossSeriesReducer
is applied to each subset of time
series. It is not possible to
reduce across different resource
types, so this field implicitly
contains resource.type. Fields not
specified in groupByFields are
aggregated away. If groupByFields
is not specified and all the time
series have the same resource
type, then the time series are
aggregated into a single output
time series. If crossSeriesReducer
is not defined, this field is
ignored.`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
												"per_series_aligner": {
													Type:         schema.TypeString,
													Optional:     true,
													ValidateFunc: verify.ValidateEnum([]string{"ALIGN_NONE", "ALIGN_DELTA", "ALIGN_RATE", "ALIGN_INTERPOLATE", "ALIGN_NEXT_OLDER", "ALIGN_MIN", "ALIGN_MAX", "ALIGN_MEAN", "ALIGN_COUNT", "ALIGN_SUM", "ALIGN_STDDEV", "ALIGN_COUNT_TRUE", "ALIGN_COUNT_FALSE", "ALIGN_FRACTION_TRUE", "ALIGN_PERCENTILE_99", "ALIGN_PERCENTILE_95", "ALIGN_PERCENTILE_50", "ALIGN_PERCENTILE_05", "ALIGN_PERCENT_CHANGE", ""}),
													Description: `The approach to be used to align
individual time series. Not all
alignment functions may be applied
to all time series, depending on
the metric type and value type of
the original time series.
Alignment may change the metric
type or the value type of the time
series.Time series data must be
aligned in order to perform cross-
time series reduction. If
crossSeriesReducer is specified,
then perSeriesAligner must be
specified and not equal ALIGN_NONE
and alignmentPeriod must be
specified; otherwise, an error is
returned. Possible values: ["ALIGN_NONE", "ALIGN_DELTA", "ALIGN_RATE", "ALIGN_INTERPOLATE", "ALIGN_NEXT_OLDER", "ALIGN_MIN", "ALIGN_MAX", "ALIGN_MEAN", "ALIGN_COUNT", "ALIGN_SUM", "ALIGN_STDDEV", "ALIGN_COUNT_TRUE", "ALIGN_COUNT_FALSE", "ALIGN_FRACTION_TRUE", "ALIGN_PERCENTILE_99", "ALIGN_PERCENTILE_95", "ALIGN_PERCENTILE_50", "ALIGN_PERCENTILE_05", "ALIGN_PERCENT_CHANGE"]`,
												},
											},
										},
									},
									"denominator_aggregations": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `Specifies the alignment of data points in
individual time series selected by
denominatorFilter as well as how to combine
the retrieved time series together (such as
when aggregating multiple streams on each
resource to a single stream for each
resource or when aggregating streams across
all members of a group of resources).When
computing ratios, the aggregations and
denominator_aggregations fields must use the
same alignment period and produce time
series that have the same periodicity and
labels.This field is similar to the one in
the MetricService.ListTimeSeries request. It
is advisable to use the ListTimeSeries
method when debugging this field.`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"alignment_period": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `The alignment period for per-time
series alignment. If present,
alignmentPeriod must be at least
60 seconds. After per-time series
alignment, each time series will
contain data points only on the
period boundaries. If
perSeriesAligner is not specified
or equals ALIGN_NONE, then this
field is ignored. If
perSeriesAligner is specified and
does not equal ALIGN_NONE, then
this field must be defined;
otherwise an error is returned.`,
												},
												"cross_series_reducer": {
													Type:             schema.TypeString,
													Optional:         true,
													ValidateFunc:     verify.ValidateEnum([]string{"REDUCE_NONE", "REDUCE_MEAN", "REDUCE_MIN", "REDUCE_MAX", "REDUCE_SUM", "REDUCE_STDDEV", "REDUCE_COUNT", "REDUCE_COUNT_TRUE", "REDUCE_COUNT_FALSE", "REDUCE_FRACTION_TRUE", "REDUCE_PERCENTILE_99", "REDUCE_PERCENTILE_95", "REDUCE_PERCENTILE_50", "REDUCE_PERCENTILE_05", ""}),
													DiffSuppressFunc: crossSeriesReducerDiffSuppress,
													Description: `The approach to be used to combine
time series. Not all reducer
functions may be applied to all
time series, depending on the
metric type and the value type of
the original time series.
Reduction may change the metric
type of value type of the time
series.Time series data must be
aligned in order to perform cross-
time series reduction. If
crossSeriesReducer is specified,
then perSeriesAligner must be
specified and not equal ALIGN_NONE
and alignmentPeriod must be
specified; otherwise, an error is
returned. Possible values: ["REDUCE_NONE", "REDUCE_MEAN", "REDUCE_MIN", "REDUCE_MAX", "REDUCE_SUM", "REDUCE_STDDEV", "REDUCE_COUNT", "REDUCE_COUNT_TRUE", "REDUCE_COUNT_FALSE", "REDUCE_FRACTION_TRUE", "REDUCE_PERCENTILE_99", "REDUCE_PERCENTILE_95", "REDUCE_PERCENTILE_50", "REDUCE_PERCENTILE_05"]`,
												},
												"group_by_fields": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `The set of fields to preserve when
crossSeriesReducer is specified.
The groupByFields determine how
the time series are partitioned
into subsets prior to applying the
aggregation function. Each subset
contains time series that have the
same value for each of the
grouping fields. Each individual
time series is a member of exactly
one subset. The crossSeriesReducer
is applied to each subset of time
series. It is not possible to
reduce across different resource
types, so this field implicitly
contains resource.type. Fields not
specified in groupByFields are
aggregated away. If groupByFields
is not specified and all the time
series have the same resource
type, then the time series are
aggregated into a single output
time series. If crossSeriesReducer
is not defined, this field is
ignored.`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
												"per_series_aligner": {
													Type:         schema.TypeString,
													Optional:     true,
													ValidateFunc: verify.ValidateEnum([]string{"ALIGN_NONE", "ALIGN_DELTA", "ALIGN_RATE", "ALIGN_INTERPOLATE", "ALIGN_NEXT_OLDER", "ALIGN_MIN", "ALIGN_MAX", "ALIGN_MEAN", "ALIGN_COUNT", "ALIGN_SUM", "ALIGN_STDDEV", "ALIGN_COUNT_TRUE", "ALIGN_COUNT_FALSE", "ALIGN_FRACTION_TRUE", "ALIGN_PERCENTILE_99", "ALIGN_PERCENTILE_95", "ALIGN_PERCENTILE_50", "ALIGN_PERCENTILE_05", "ALIGN_PERCENT_CHANGE", ""}),
													Description: `The approach to be used to align
individual time series. Not all
alignment functions may be applied
to all time series, depending on
the metric type and value type of
the original time series.
Alignment may change the metric
type or the value type of the time
series.Time series data must be
aligned in order to perform cross-
time series reduction. If
crossSeriesReducer is specified,
then perSeriesAligner must be
specified and not equal ALIGN_NONE
and alignmentPeriod must be
specified; otherwise, an error is
returned. Possible values: ["ALIGN_NONE", "ALIGN_DELTA", "ALIGN_RATE", "ALIGN_INTERPOLATE", "ALIGN_NEXT_OLDER", "ALIGN_MIN", "ALIGN_MAX", "ALIGN_MEAN", "ALIGN_COUNT", "ALIGN_SUM", "ALIGN_STDDEV", "ALIGN_COUNT_TRUE", "ALIGN_COUNT_FALSE", "ALIGN_FRACTION_TRUE", "ALIGN_PERCENTILE_99", "ALIGN_PERCENTILE_95", "ALIGN_PERCENTILE_50", "ALIGN_PERCENTILE_05", "ALIGN_PERCENT_CHANGE"]`,
												},
											},
										},
									},
									"denominator_filter": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `A filter that identifies a time series that
should be used as the denominator of a ratio
that will be compared with the threshold. If
a denominator_filter is specified, the time
series specified by the filter field will be
used as the numerator.The filter is similar
to the one that is specified in the
MetricService.ListTimeSeries request (that
call is useful to verify the time series
that will be retrieved / processed) and must
specify the metric type and optionally may
contain restrictions on resource type,
resource labels, and metric labels. This
field may not exceed 2048 Unicode characters
in length.`,
									},
									"evaluation_missing_data": {
										Type:         schema.TypeString,
										Optional:     true,
										ValidateFunc: verify.ValidateEnum([]string{"EVALUATION_MISSING_DATA_INACTIVE", "EVALUATION_MISSING_DATA_ACTIVE", "EVALUATION_MISSING_DATA_NO_OP", ""}),
										Description: `A condition control that determines how
metric-threshold conditions are evaluated when
data stops arriving. Possible values: ["EVALUATION_MISSING_DATA_INACTIVE", "EVALUATION_MISSING_DATA_ACTIVE", "EVALUATION_MISSING_DATA_NO_OP"]`,
									},
									"filter": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `A filter that identifies which time series
should be compared with the threshold.The
filter is similar to the one that is
specified in the
MetricService.ListTimeSeries request (that
call is useful to verify the time series
that will be retrieved / processed) and must
specify the metric type and optionally may
contain restrictions on resource type,
resource labels, and metric labels. This
field may not exceed 2048 Unicode characters
in length.`,
									},
									"forecast_options": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `When this field is present, the 'MetricThreshold'
condition forecasts whether the time series is
predicted to violate the threshold within the
'forecastHorizon'. When this field is not set, the
'MetricThreshold' tests the current value of the
timeseries against the threshold.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"forecast_horizon": {
													Type:     schema.TypeString,
													Required: true,
													Description: `The length of time into the future to forecast
whether a timeseries will violate the threshold.
If the predicted value is found to violate the
threshold, and the violation is observed in all
forecasts made for the Configured 'duration',
then the timeseries is considered to be failing.`,
												},
											},
										},
									},
									"threshold_value": {
										Type:     schema.TypeFloat,
										Optional: true,
										Description: `A value against which to compare the time
series.`,
									},
									"trigger": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `The number/percent of time series for which
the comparison must hold in order for the
condition to trigger. If unspecified, then
the condition will trigger if the comparison
is true for any of the time series that have
been identified by filter and aggregations,
or by the ratio, if denominator_filter and
denominator_aggregations are specified.`,
										MaxItems: 1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"count": {
													Type:     schema.TypeInt,
													Optional: true,
													Description: `The absolute number of time series
that must fail the predicate for the
condition to be triggered.`,
												},
												"percent": {
													Type:     schema.TypeFloat,
													Optional: true,
													Description: `The percentage of time series that
must fail the predicate for the
condition to be triggered.`,
												},
											},
										},
									},
								},
							},
						},
					},
				},
			},
			"display_name": {
				Type:     schema.TypeString,
				Required: true,
				Description: `A short name or phrase used to identify the policy in
dashboards, notifications, and incidents. To avoid confusion, don't use
the same display name for multiple policies in the same project. The
name is limited to 512 Unicode characters.`,
			},
			"alert_strategy": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `Control over how this alert policy's notification channels are notified.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"auto_close": {
							Type:        schema.TypeString,
							Optional:    true,
							Description: `If an alert policy that was active has no data for this long, any open incidents will close.`,
						},
						"notification_channel_strategy": {
							Type:     schema.TypeList,
							Optional: true,
							Description: `Control over how the notification channels in 'notification_channels'
are notified when this alert fires, on a per-channel basis.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"notification_channel_names": {
										Type:     schema.TypeList,
										Optional: true,
										Description: `The notification channels that these settings apply to. Each of these
correspond to the name field in one of the NotificationChannel objects
referenced in the notification_channels field of this AlertPolicy. The format is
'projects/[PROJECT_ID_OR_NUMBER]/notificationChannels/[CHANNEL_ID]'`,
										Elem: &schema.Schema{
											Type: schema.TypeString,
										},
									},
									"renotify_interval": {
										Type:        schema.TypeString,
										Optional:    true,
										Description: `The frequency at which to send reminder notifications for open incidents.`,
									},
								},
							},
						},
						"notification_prompts": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `Control when notifications will be sent out. Possible values: ["NOTIFICATION_PROMPT_UNSPECIFIED", "OPENED", "CLOSED"]`,
							Elem: &schema.Schema{
								Type:         schema.TypeString,
								ValidateFunc: verify.ValidateEnum([]string{"NOTIFICATION_PROMPT_UNSPECIFIED", "OPENED", "CLOSED"}),
							},
						},
						"notification_rate_limit": {
							Type:     schema.TypeList,
							Optional: true,
							Description: `Required for alert policies with a LogMatch condition.
This limit is not implemented for alert policies that are not log-based.`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"period": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `Not more than one notification per period.
A duration in seconds with up to nine fractional digits, terminated by 's'. Example "60.5s".`,
									},
								},
							},
						},
					},
				},
			},
			"documentation": {
				Type:     schema.TypeList,
				Optional: true,
				Description: `Documentation that is included with notifications and incidents related
to this policy. Best practice is for the documentation to include information
to help responders understand, mitigate, escalate, and correct the underlying
problems detected by the alerting policy. Notification channels that have
limited capacity might not show this documentation.`,
				MaxItems: 1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"content": {
							Type:     schema.TypeString,
							Optional: true,
							Description: `The text of the documentation, interpreted according to mimeType.
The content may not exceed 8,192 Unicode characters and may not
exceed more than 10,240 bytes when encoded in UTF-8 format,
whichever is smaller.`,
							AtLeastOneOf: []string{"documentation.0.content", "documentation.0.links", "documentation.0.mime_type", "documentation.0.subject"},
						},
						"links": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `Links to content such as playbooks, repositories, and other resources. This field can contain up to 3 entries.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"display_name": {
										Type:        schema.TypeString,
										Optional:    true,
										Description: `A short display name for the link. The display name must not be empty or exceed 63 characters. Example: "playbook".`,
									},
									"url": {
										Type:        schema.TypeString,
										Optional:    true,
										Description: `The url of a webpage. A url can be templatized by using variables in the path or the query parameters. The total length of a URL should not exceed 2083 characters before and after variable expansion. Example: "https://my_domain.com/playbook?name=${resource.name}".`,
									},
								},
							},
							AtLeastOneOf: []string{"documentation.0.content", "documentation.0.links", "documentation.0.mime_type", "documentation.0.subject"},
						},
						"mime_type": {
							Type:     schema.TypeString,
							Optional: true,
							Description: `The format of the content field. Presently, only the value
"text/markdown" is supported.`,
							Default:      "text/markdown",
							AtLeastOneOf: []string{"documentation.0.content", "documentation.0.links", "documentation.0.mime_type", "documentation.0.subject"},
						},
						"subject": {
							Type:     schema.TypeString,
							Optional: true,
							Description: `The subject line of the notification. The subject line may not
exceed 10,240 bytes. In notifications generated by this policy the contents
of the subject line after variable expansion will be truncated to 255 bytes
or shorter at the latest UTF-8 character boundary.`,
							AtLeastOneOf: []string{"documentation.0.content", "documentation.0.links", "documentation.0.mime_type", "documentation.0.subject"},
						},
					},
				},
			},
			"enabled": {
				Type:        schema.TypeBool,
				Optional:    true,
				Description: `Whether or not the policy is enabled. The default is true.`,
				Default:     true,
			},
			"notification_channels": {
				Type:     schema.TypeList,
				Optional: true,
				Description: `Identifies the notification channels to which notifications should be
sent when incidents are opened or closed or when new violations occur
on an already opened incident. Each element of this array corresponds
to the name field in each of the NotificationChannel objects that are
returned from the notificationChannels.list method. The syntax of the
entries in this field is
'projects/[PROJECT_ID]/notificationChannels/[CHANNEL_ID]'`,
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"severity": {
				Type:         schema.TypeString,
				Optional:     true,
				ValidateFunc: verify.ValidateEnum([]string{"CRITICAL", "ERROR", "WARNING", ""}),
				Description: `The severity of an alert policy indicates how important incidents generated
by that policy are. The severity level will be displayed on the Incident
detail page and in notifications. Possible values: ["CRITICAL", "ERROR", "WARNING"]`,
			},
			"user_labels": {
				Type:     schema.TypeMap,
				Optional: true,
				Description: `This field is intended to be used for organizing and identifying the AlertPolicy
objects.The field can contain up to 64 entries. Each key and value is limited
to 63 Unicode characters or 128 bytes, whichever is smaller. Labels and values
can contain only lowercase letters, numerals, underscores, and dashes. Keys
must begin with a letter.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"creation_record": {
				Type:     schema.TypeList,
				Computed: true,
				Description: `A read-only record of the creation of the alerting policy.
If provided in a call to create or update, this field will
be ignored.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{},
				},
			},
			"name": {
				Type:     schema.TypeString,
				Computed: true,
				Description: `The unique resource name for this policy.
Its syntax is: projects/[PROJECT_ID]/alertPolicies/[ALERT_POLICY_ID]`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}
